{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBLOLiBvnAb"
      },
      "source": [
        "# GPT From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19sZEdjvDIQk"
      },
      "source": [
        "# Encoder - Decoder Transformer\n",
        "An Encoder–Decoder Transformer is the original Transformer architecture introduced in Attention Is All You Need (Vaswani et al., 2017). It is designed to model conditional sequence generation.\n",
        "\n",
        "## Why encoder–decoder lost dominance\n",
        "\n",
        "Encoder–decoder models were very strong around 2018–2020. Decoder-only models won out because they:\n",
        "\n",
        "- Scale better (simpler architecture, fewer attention paths)\n",
        "\n",
        "- Have perfect training–inference alignment\n",
        "\n",
        "- Can absorb conditional tasks via prompting\n",
        "\n",
        "- Are easier to use as general-purpose models\n",
        "\n",
        "- Enable efficient KV caching during generation\n",
        "\n",
        "In practice, decoder-only models learned to simulate encoder–decoder behavior by treating the “input” as part of the prefix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asgn5G5q17xv"
      },
      "source": [
        "## Architecture\n",
        "![Arch](https://media.geeksforgeeks.org/wp-content/uploads/20240524112538/Screenshot-2024-05-24-112357-768.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdegdYJ92cmf"
      },
      "source": [
        "## Positional Embeddings\n",
        "Positional embeddings add explicit numerical information about the order (position) of elements in a sequence to their vector representations. This is necessary because Transformer models process all tokens in parallel using self-attention and therefore have no inherent notion of sequence order. Without positional information, a Transformer would treat a sentence as a set of tokens rather than an ordered sequence.\n",
        "\n",
        "To solve this, each position in a sequence (e.g., 1st word, 2nd word, 3rd word, etc.) is assigned a position-specific vector. This positional vector is added to the token’s embedding before being passed into the model. As a result, the final input representation contains both:\n",
        "\n",
        "- What the token is (semantic content from the word embedding), and\n",
        "\n",
        "- Where the token appears (its position in the sequence).\n",
        "\n",
        "This allows the model to distinguish between sequences with the same words but different orderings, such as “dog bites man” versus “man bites dog”, where meaning critically depends on position.\n",
        "\n",
        "The example shows fixed, absolute sinusoidal positional embeddings as introduced in Attention Is All You Need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gh9VceQRvSyX"
      },
      "outputs": [],
      "source": [
        "# Position Encoding\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()  # Initialize nn.Module internals\n",
        "\n",
        "        # Create a (max_len x d_model) matrix to store positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # Create position indices: [0, 1, 2, ..., max_len-1]\n",
        "        # Shape: (max_len, 1)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Compute the frequency scaling term for each even dimension\n",
        "        # Shape: (d_model / 2,)\n",
        "        #\n",
        "        # Each pair of embedding dimensions (2i, 2i+1) shares a unique frequency.\n",
        "        # These frequencies are exponentially spaced so that:\n",
        "        #   - Early dimensions oscillate quickly (high-frequency signals),\n",
        "        #     capturing fine-grained, local positional differences.\n",
        "        #   - Later dimensions oscillate slowly (low-frequency signals),\n",
        "        #     capturing long-range, global positional structure.\n",
        "        #\n",
        "        # This multi-scale design allows the model to represent both short- and\n",
        "        # long-distance relationships between tokens.\n",
        "        #\n",
        "        # The exponential scaling (10000^(-2i / d_model)) ensures:\n",
        "        #   - Positional encodings remain unique across long sequences\n",
        "        #   - Relative position information can be recovered using linear operations,\n",
        "        #     which is critical for attention mechanisms.\n",
        "        #\n",
        "        # Using fixed (non-learned) frequencies avoids overfitting to training\n",
        "        # sequence lengths and enables generalization to longer sequences.\n",
        "\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # Apply sine to even embedding dimensions (0, 2, 4, ...)\n",
        "        # Broadcasting: (max_len, 1) * (d_model/2,) -> (max_len, d_model/2)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # If positional encodings were different per batch element, the model could learn:\n",
        "        # “this sentence is in batch slot 3”\n",
        "        # “batch index correlates with label”\n",
        "        # That would be spurious information and break generalization. Hence we ensure: No batch-specific signal and that only relative token order is encoded\n",
        "        # Apply cosine to odd embedding dimensions (1, 3, 5, ...)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension so it can be broadcast across batches\n",
        "        # Final shape: (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input embeddings of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            Embeddings with positional information added\n",
        "        \"\"\"\n",
        "        # Slice positional encodings to match sequence length\n",
        "        # Broadcasting adds the same positional encoding to each batch element\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duoDvpEz3MDL"
      },
      "source": [
        "\n",
        "Importantly, positional embeddings depend only on the token’s position within a sequence, not on which sequence in the batch it belongs to. Therefore, the same positional embedding for position t is shared across all sequences in a batch and is broadcast during computation. This ensures that positional information encodes relative and absolute order, rather than introducing spurious batch-specific signals.\n",
        "\n",
        "There are 2 types of positional embeddinngs\n",
        "\n",
        "- Fixed (sinusoidal), where positions are encoded using sine and cosine functions of different frequencies, enabling the model to generalize to longer sequences and infer relative distances between tokens.\n",
        "\n",
        "- Learned, where positional vectors are trained parameters similar to word embeddings.\n",
        "\n",
        "In both cases, positional embeddings are combined with token embeddings via element-wise addition, preserving dimensionality while enriching each token representation with sequence order information. This simple design enables Transformers to model syntax, dependencies, and context over sequences effectively, despite their parallel processing nature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzbWlxDf4fD8"
      },
      "source": [
        "\n",
        "\n",
        "## Scaled Dot-Product Attention\n",
        "\n",
        "Scaled dot-product attention is the core mechanism that allows Transformer models to **selectively focus on the most relevant parts of a sequence** when processing each token. Instead of treating all tokens equally, attention computes how strongly one token should “attend” to every other token based on learned representations.\n",
        "\n",
        "Each token is represented using three vectors:\n",
        "\n",
        "* **Query (Q):** what this token is looking for\n",
        "* **Key (K):** what each token offers\n",
        "* **Value (V):** the information each token contains\n",
        "\n",
        "Attention works by comparing each query with all keys using a dot product, producing a set of similarity scores that measure relevance. A higher score means the key token is more relevant to the query token. These scores are then normalized into a probability distribution and used to compute a weighted sum of the value vectors, producing a context-aware representation of each token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ok_4uparweJb"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super().__init__()\n",
        "\n",
        "        # Scaling factor = sqrt(d_k)\n",
        "        #\n",
        "        # The dot product Q · K grows in magnitude with the dimensionality d_k.\n",
        "        # Without scaling, large dot products would push the softmax into\n",
        "        # extremely peaked distributions, causing:\n",
        "        #   - Vanishing gradients\n",
        "        #   - Overconfident, brittle attention weights\n",
        "        #\n",
        "        # Dividing by sqrt(d_k) keeps the variance of the scores roughly constant,\n",
        "        # stabilizing training and ensuring smoother attention distributions.\n",
        "        self.scale = math.sqrt(d_k)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q: Queries of shape (..., seq_len_q, d_k)\n",
        "            K: Keys of shape (..., seq_len_k, d_k)\n",
        "            V: Values of shape (..., seq_len_k, d_v)\n",
        "            mask: Optional attention mask (broadcastable to scores shape)\n",
        "                  Used to block padding tokens or future tokens (causal masking)\n",
        "\n",
        "        Returns:\n",
        "            output: Attention-weighted values\n",
        "            attn: Attention weight matrix\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute raw attention scores via dot product:\n",
        "        #   score_{i,j} = Q_i · K_j\n",
        "        #\n",
        "        # K is transposed so that:\n",
        "        #   (..., seq_len_q, d_k) @ (..., d_k, seq_len_k)\n",
        "        # → (..., seq_len_q, seq_len_k)\n",
        "        #\n",
        "        # Each score measures how much token i should attend to token j.\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        # Apply mask (if provided) to prevent attention to certain positions:\n",
        "        #   - Padding tokens (in encoder attention)\n",
        "        #   - Future tokens (in decoder self-attention)\n",
        "        #\n",
        "        # Masked positions are set to a large negative value so that\n",
        "        # softmax assigns them near-zero probability.\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Normalize scores into a probability distribution using softmax.\n",
        "        # For each query token, attention weights over all key positions sum to 1.\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute the final attention output as a weighted sum of values:\n",
        "        #   output_i = Σ_j attn_{i,j} · V_j\n",
        "        #\n",
        "        # This mixes information from all tokens, weighted by relevance.\n",
        "        return torch.matmul(attn, V), attn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7nCZPJX47ld"
      },
      "source": [
        "\n",
        "A critical design choice is the **scaling factor**. As the dimensionality of the key vectors increases, the magnitude of dot products grows, which would cause the softmax function to become overly peaked. This would make the model excessively confident in a small number of tokens and lead to vanishing gradients during training. Dividing by sqrt(d_K) stabilizes the distribution of attention scores, ensuring smooth gradients and more balanced attention across tokens.\n",
        "\n",
        "Another key design element is **masking**, which allows the model to control where attention is permitted. Masks prevent attention to padding tokens (which carry no information) and, in autoregressive settings, prevent tokens from attending to future positions. This enforces the correct information flow while keeping the attention mechanism fully parallelizable.\n",
        "\n",
        "By combining similarity-based scoring, variance-stabilizing scaling, and flexible masking, scaled dot-product attention enables Transformers to model long-range dependencies, contextual meaning, and structured information flow efficiently. Importantly, this mechanism is fully differentiable, parallelizable, and independent of sequence length assumptions, making it a foundational building block of modern sequence models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eFd83e95XnN"
      },
      "source": [
        "## MultiHeadAttention\n",
        "\n",
        "Multi-head attention extends scaled dot-product attention by allowing the model to attend to different types of relationships simultaneously. Instead of computing a single attention distribution over the entire embedding space, the model splits the representation into multiple lower-dimensional subspaces called heads and performs attention independently in each one.\n",
        "\n",
        "Each head learns to focus on a different aspect of the sequence. For example, one head may specialize in short-range syntactic relationships (such as subject–verb agreement), while another may focus on long-range semantic dependencies or positional structure. This decomposition allows the model to capture richer and more diverse patterns than a single attention mechanism could."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u-Uh-mqww6MX"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # d_model must be divisible by the number of heads\n",
        "        # because we split the embedding space evenly across heads.\n",
        "        # Each head operates on a lower-dimensional subspace.\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        # Dimensionality per head (d_k)\n",
        "        # Total dimension is preserved when heads are concatenated back.\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Linear projections for Queries, Keys, and Values\n",
        "        #\n",
        "        # These allow the model to learn different representations\n",
        "        # of the same input depending on how it is being used:\n",
        "        #   - As a query (what am I looking for?)\n",
        "        #   - As a key   (what do I offer?)\n",
        "        #   - As a value (what information do I carry?)\n",
        "        #\n",
        "        # Each projects from d_model → d_model, after which we split\n",
        "        # into multiple heads.\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        #\n",
        "        # After attention is computed independently in each head,\n",
        "        # their outputs are concatenated and mixed using this layer.\n",
        "        # This allows information from different heads to interact.\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Scaled dot-product attention applied independently per head\n",
        "        self.attn = ScaledDotProductAttention(self.d_k)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q, K, V: Input tensors of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        B, T, _ = Q.shape\n",
        "\n",
        "        # Project inputs into Q, K, V spaces\n",
        "        # Shape after projection: (B, T, d_model)\n",
        "        #\n",
        "        # Then reshape to split into multiple heads:\n",
        "        #   (B, T, n_heads, d_k)\n",
        "        #\n",
        "        # Finally transpose so that heads become a separate dimension:\n",
        "        #   (B, n_heads, T, d_k)\n",
        "        #\n",
        "        # This allows attention to be computed independently per head.\n",
        "        Q = self.W_q(Q).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention to each head in parallel\n",
        "        # Output shape: (B, n_heads, T, d_k)\n",
        "        x, _ = self.attn(Q, K, V, mask)\n",
        "\n",
        "        # Recombine heads:\n",
        "        #   (B, n_heads, T, d_k) → (B, T, n_heads * d_k)\n",
        "        #\n",
        "        # .contiguous() ensures memory layout is correct before reshaping\n",
        "        x = x.transpose(1, 2).contiguous().view(B, T, -1)\n",
        "\n",
        "        # Final linear projection mixes information across heads\n",
        "        return self.W_o(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VGW-Hoh5cHf"
      },
      "source": [
        "To enable this, the input embeddings are first projected into separate query, key, and value spaces using learned linear transformations. These projections are then reshaped so that each head operates on a reduced dimensionality. Attention is computed independently within each head using scaled dot-product attention, producing multiple parallel context representations.\n",
        "\n",
        "After attention is applied, the outputs of all heads are concatenated and passed through a final linear projection. This step is crucial: it allows information from different heads to be combined and re-integrated into a single representation, enabling interaction between the different relational perspectives learned by each head.\n",
        "\n",
        "A key design choice is that splitting into multiple heads does not increase computational cost relative to a single large attention operation. The total dimensionality remains constant; it is simply redistributed across heads. This makes multi-head attention both expressive and efficient.\n",
        "\n",
        "Overall, multi-head attention gives the Transformer the ability to look at the same sequence through multiple lenses at once, making it far more powerful at modeling complex structure, long-range dependencies, and contextual meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTeCGT5h5wzN"
      },
      "source": [
        "\n",
        "## FeedForward Network (Position-wise FFN)\n",
        "\n",
        "### 1. Why do we need a FeedForward network at all?\n",
        "\n",
        "Self-attention **mixes information across tokens**, but it is largely a *linear* operation with respect to feature dimensions. Without an additional non-linear component, stacking attention layers would still result in a model that is limited in its ability to learn complex transformations.\n",
        "\n",
        "This motivation is stated explicitly in **Vaswani et al., 2017 (*Attention Is All You Need*)**, where each attention sub-layer is followed by a *position-wise fully connected feed-forward network* to introduce non-linearity and increase representational power.\n",
        "\n",
        "The FeedForward layer:\n",
        "\n",
        "* Operates **independently on each token** (position-wise)\n",
        "* Mixes information **across features**, not across time\n",
        "* Acts like a learned feature transformer at each position\n",
        "\n",
        "You can think of attention as answering *“where should I look?”*\n",
        "and the feedforward network as answering *“how should I process what I found?”*\n",
        "\n",
        "This division of labor—**global interaction via attention, local transformation via FFN**—is a core architectural principle of Transformers.\n",
        "\n",
        "**Reference:**\n",
        "Vaswani et al., *Attention Is All You Need*, NeurIPS 2017\n",
        "\n",
        "### 2. Why expand to `d_ff` and then project back?\n",
        "\n",
        "This is a deliberate **capacity expansion strategy**, introduced in the original Transformer.\n",
        "\n",
        "In Vaswani et al. (2017), the feedforward network is defined as:\n",
        "\n",
        "[\n",
        "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "]\n",
        "\n",
        "where ( W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}} ) and\n",
        "( W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}} ).\n",
        "\n",
        "The reasoning:\n",
        "\n",
        "* Expanding to a higher-dimensional space (`d_ff ≫ d_model`) allows the model to represent **richer intermediate features**.\n",
        "* Projecting back to `d_model` keeps the interface between layers consistent, enabling deep stacking.\n",
        "\n",
        "This mirrors a well-established pattern in deep learning:\n",
        "\n",
        "> temporary expansion → nonlinear processing → compression\n",
        "\n",
        "Similar ideas appear in:\n",
        "\n",
        "* Bottleneck layers in CNNs (He et al., ResNet)\n",
        "* Inverted residuals in MobileNet\n",
        "* MLP blocks in Vision Transformers (Dosovitskiy et al., 2020)\n",
        "\n",
        "**Reference:**\n",
        "Vaswani et al., 2017\n",
        "Dosovitskiy et al., *An Image is Worth 16×16 Words*, ICLR 2021\n",
        "\n",
        "### 3. Why residual connections everywhere?\n",
        "\n",
        "Residual connections were introduced to address the **optimization difficulty of deep networks** (He et al., 2016) and were adopted wholesale in Transformers.\n",
        "\n",
        "In the Transformer, **every sub-layer** (attention and feedforward) is wrapped with a residual connection:\n",
        "\n",
        "[\n",
        "x \\rightarrow x + \\text{sublayer}(x)\n",
        "]\n",
        "\n",
        "This ensures that:\n",
        "\n",
        "* The model can easily learn **identity mappings**\n",
        "* Gradients flow effectively through many stacked layers\n",
        "* Deeper models remain trainable and stable\n",
        "\n",
        "Vaswani et al. explicitly note that residual connections are critical for training deep attention-based architectures.\n",
        "\n",
        "Formally, residuals allow layers to learn **incremental refinements** rather than complete transformations, which significantly improves optimization.\n",
        "\n",
        "**Reference:**\n",
        "He et al., *Deep Residual Learning for Image Recognition*, CVPR 2016\n",
        "Vaswani et al., 2017\n",
        "\n",
        "### 4. Why LayerNorm after each sub-layer?\n",
        "\n",
        "Layer Normalization was chosen because:\n",
        "\n",
        "* It normalizes **per token**, not across the batch\n",
        "* It works well with variable sequence lengths\n",
        "* It is stable under parallel computation\n",
        "\n",
        "In the original Transformer, LayerNorm is applied **after** the residual connection (post-norm):\n",
        "\n",
        "```python\n",
        "x = LayerNorm(x + sublayer(x))\n",
        "```\n",
        "\n",
        "LayerNorm helps:\n",
        "\n",
        "* Reduce internal covariate shift\n",
        "* Prevent exploding or vanishing activations\n",
        "* Stabilize training across layers\n",
        "\n",
        "Later work (e.g., **Pre-LN Transformers**) showed that moving LayerNorm *before* the sub-layer improves gradient flow in very deep models, but the **core motivation remains unchanged**.\n",
        "\n",
        "**References:**\n",
        "Ba et al., *Layer Normalization*, arXiv 2016\n",
        "Vaswani et al., 2017\n",
        "Xiong et al., *On Layer Normalization in Transformers*, ICML 2020\n",
        "\n",
        "### 5. Why split attention and feedforward into two sub-layers?\n",
        "\n",
        "This separation is a **deliberate inductive bias** introduced in the original Transformer architecture.\n",
        "\n",
        "Each Encoder block alternates between:\n",
        "\n",
        "1. **Context aggregation** (self-attention)\n",
        "2. **Feature transformation** (position-wise feedforward)\n",
        "\n",
        "This enforces a clean conceptual separation:\n",
        "\n",
        "* Attention decides **which tokens influence each other**\n",
        "* Feedforward decides **how each token representation should be transformed**\n",
        "\n",
        "Stacking these blocks allows:\n",
        "\n",
        "* Global information to propagate through attention\n",
        "* Local, non-linear refinement at each layer\n",
        "\n",
        "This alternating structure is one of the key reasons Transformers scale so well with depth and data.\n",
        "\n",
        "**Reference:**\n",
        "Vaswani et al., 2017\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "znGjpxeDw_6d"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        # Position-wise FeedForward Network\n",
        "        #\n",
        "        # This is applied independently to each token position.\n",
        "        # It does NOT mix information across time (sequence length),\n",
        "        # only across feature dimensions.\n",
        "        #\n",
        "        # The structure expands the representation into a higher-\n",
        "        # dimensional space (d_ff), applies a non-linearity,\n",
        "        # and then projects it back to d_model.\n",
        "        #\n",
        "        # This allows the model to:\n",
        "        #   - Introduce non-linear transformations\n",
        "        #   - Increase representational capacity\n",
        "        #   - Learn complex feature interactions per token\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),  # Expansion (feature mixing)\n",
        "            nn.ReLU(),                 # Non-linearity\n",
        "            nn.Linear(d_ff, d_model)   # Compression back to model dimension\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        # FeedForward is applied identically to every token position\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention:\n",
        "        # Allows each token to attend to all other tokens in the sequence\n",
        "        # and build context-aware representations.\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Position-wise feedforward network:\n",
        "        # Adds non-linear transformation capacity after attention.\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization layers\n",
        "        #\n",
        "        # These stabilize training by normalizing feature distributions\n",
        "        # and help gradient flow in deep transformer stacks.\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        # Prevents overfitting and co-adaptation of neurons.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Self-Attention Sub-layer ---\n",
        "        #\n",
        "        # Residual connection:\n",
        "        #   Preserves the original representation and improves gradient flow.\n",
        "        #\n",
        "        # Dropout:\n",
        "        #   Regularizes attention outputs.\n",
        "        #\n",
        "        # LayerNorm (post-norm formulation here):\n",
        "        #   Stabilizes activations and training dynamics.\n",
        "        x = self.norm1(\n",
        "            x + self.dropout(self.attn(x, x, x, mask))\n",
        "        )\n",
        "\n",
        "        # --- FeedForward Sub-layer ---\n",
        "        #\n",
        "        # Again, we apply:\n",
        "        #   - Position-wise feedforward transformation\n",
        "        #   - Dropout for regularization\n",
        "        #   - Residual connection to preserve information\n",
        "        #   - LayerNorm for stability\n",
        "        x = self.norm2(\n",
        "            x + self.dropout(self.ff(x))\n",
        "        )\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0OwolRl6H0Y"
      },
      "source": [
        "The Transformer encoder block combines ideas from residual learning, normalization, and MLP expansion into a modular design where **attention handles interaction** and **feedforward layers handle transformation**. This structure, first formalized in *Attention Is All You Need*, remains largely unchanged across modern large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNtVsxs561am"
      },
      "source": [
        "\n",
        "## Decoder Block — Why it exists and how it works\n",
        "\n",
        "### 1. Why does the decoder have *masked* self-attention?\n",
        "\n",
        "Unlike the encoder, the decoder is used for **autoregressive generation**.\n",
        "When predicting token *t*, the model must not access tokens *t+1, t+2, …*.\n",
        "\n",
        "This constraint is enforced using a **causal (look-ahead) mask** in self-attention, as introduced in **Vaswani et al., 2017**. The mask ensures that each position can only attend to earlier positions, preserving the correct probabilistic factorization.\n",
        "\n",
        "Without masked self-attention, the decoder would leak future information and collapse training.\n",
        "\n",
        "\n",
        "### 2. Why do we need cross-attention at all?\n",
        "\n",
        "Cross-attention is what allows the decoder to **condition on the source sequence**.\n",
        "\n",
        "* Queries come from the decoder (current generation state)\n",
        "* Keys and values come from the encoder (encoded source information)\n",
        "\n",
        "This mechanism allows the decoder to dynamically decide:\n",
        "\n",
        "> “Which parts of the input sequence are relevant *right now*?”\n",
        "\n",
        "This design replaces fixed alignment mechanisms used in earlier sequence-to-sequence models (e.g., RNN encoder–decoders) with a fully differentiable, parallelizable alternative.\n",
        "\n",
        "**Reference:**\n",
        "Bahdanau et al., *Neural Machine Translation by Jointly Learning to Align and Translate*, 2015\n",
        "Vaswani et al., 2017\n",
        "\n",
        "\n",
        "### 3. Why is self-attention applied *before* cross-attention?\n",
        "\n",
        "This ordering is intentional and appears in the original Transformer.\n",
        "\n",
        "The decoder first:\n",
        "\n",
        "1. Builds a representation of **what has been generated so far** (self-attention)\n",
        "2. Then aligns that representation with the **source sequence** (cross-attention)\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "* Self-attention answers: *“What have I already said?”*\n",
        "* Cross-attention answers: *“What does the input tell me to say next?”*\n",
        "\n",
        "Reversing this order degrades performance, as the decoder would try to attend to the source without a coherent internal state.\n",
        "\n",
        "\n",
        "### 4. Why does the decoder also need a FeedForward network?\n",
        "\n",
        "Just like in the encoder, attention alone is insufficient.\n",
        "\n",
        "* Attention mixes information across tokens\n",
        "* FeedForward networks introduce **non-linearity and feature transformation**\n",
        "\n",
        "In the decoder, the FFN refines the token representation *after* both:\n",
        "\n",
        "* past context (self-attention), and\n",
        "* source context (cross-attention)\n",
        "\n",
        "This ensures expressive power at each decoding step.\n",
        "\n",
        "\n",
        "### 5. Why three residual + normalization blocks?\n",
        "\n",
        "Each sub-layer (self-attn, cross-attn, FFN) performs a fundamentally different operation and has different activation statistics. Wrapping each with its own residual connection and LayerNorm:\n",
        "\n",
        "* Stabilizes training\n",
        "* Preserves information flow\n",
        "* Allows deep stacking of decoder blocks\n",
        "\n",
        "This mirrors the encoder design but extends it to handle **conditional generation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wvL-9orJ5hta"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Masked multi-head self-attention\n",
        "        #\n",
        "        # This allows each target token to attend only to:\n",
        "        #   - itself\n",
        "        #   - previous target tokens\n",
        "        #\n",
        "        # The causal (look-ahead) mask ensures autoregressive behavior:\n",
        "        # the model cannot \"see the future\" during generation.\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Cross-attention (encoder–decoder attention)\n",
        "        #\n",
        "        # Queries come from the decoder (what do I need?),\n",
        "        # Keys and Values come from the encoder (what information is available?).\n",
        "        #\n",
        "        # This is how the decoder conditions its predictions on the source sequence.\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Position-wise feedforward network\n",
        "        #\n",
        "        # Applies non-linear transformation independently to each target token\n",
        "        # after contextual information has been integrated.\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # LayerNorm layers for each sub-layer\n",
        "        #\n",
        "        # Separate norms are used because each sub-layer has\n",
        "        # different statistical properties.\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, src_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Decoder input embeddings (batch_size, tgt_len, d_model)\n",
        "            enc_out: Encoder outputs (batch_size, src_len, d_model)\n",
        "            tgt_mask: Causal mask to block future target tokens\n",
        "            src_mask: Mask to block padding in the source sequence\n",
        "\n",
        "        Returns:\n",
        "            Decoder output representations\n",
        "        \"\"\"\n",
        "\n",
        "        # Allows each target token to attend only to earlier target tokens.\n",
        "        # This enforces the autoregressive property required for generation.\n",
        "        #\n",
        "        # Residual connection + dropout + LayerNorm stabilize training\n",
        "        # and preserve the original token representation.\n",
        "        x = self.norm1(\n",
        "            x + self.dropout(self.self_attn(x, x, x, tgt_mask))\n",
        "        )\n",
        "\n",
        "        # Decoder queries attend over encoder keys and values.\n",
        "        # This is the information bottleneck where the decoder\n",
        "        # decides which parts of the source sequence are relevant\n",
        "        # for predicting the next token.\n",
        "        x = self.norm2(\n",
        "            x + self.dropout(self.cross_attn(x, enc_out, enc_out, src_mask))\n",
        "        )\n",
        "\n",
        "        # Applies a non-linear, position-wise transformation\n",
        "        # to refine the decoder representations after all\n",
        "        # contextual information has been integrated.\n",
        "        x = self.norm3(\n",
        "            x + self.dropout(self.ff(x))\n",
        "        )\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6cLt14t6-it"
      },
      "source": [
        "The decoder first reasons about what it has already generated, then selectively consults the source sequence, and finally refines the result through non-linear transformation — all while strictly preventing future information leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEniZj8g7YIN"
      },
      "source": [
        "\n",
        "## Transformer — Why this architecture works\n",
        "\n",
        "### 1. Why separate encoder and decoder?\n",
        "\n",
        "The Transformer is designed for **sequence-to-sequence** tasks (e.g., translation).\n",
        "\n",
        "* The **encoder** reads and understands the entire input sequence.\n",
        "* The **decoder** generates the output sequence one token at a time, conditioned on:\n",
        "\n",
        "  * what it has already generated, and\n",
        "  * the encoder’s representation of the input.\n",
        "\n",
        "This separation cleanly mirrors the probabilistic goal:\n",
        "\n",
        "> Generate an output sequence conditioned on an input sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Why stack multiple layers?\n",
        "\n",
        "A single attention layer can only perform **one round of information exchange**.\n",
        "\n",
        "By stacking layers:\n",
        "\n",
        "* Early layers capture local or shallow relationships\n",
        "* Later layers build more abstract, global representations\n",
        "\n",
        "This hierarchical refinement is analogous to depth in CNNs and deep MLPs.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Why positional encoding at the input?\n",
        "\n",
        "Attention alone has **no notion of order**.\n",
        "Without positional encodings:\n",
        "\n",
        "```\n",
        "[\"dog\", \"bites\", \"man\"]\n",
        "[\"man\", \"bites\", \"dog\"]\n",
        "```\n",
        "\n",
        "would look identical to the model.\n",
        "\n",
        "Adding positional encodings at the input ensures that **order information flows through every layer** of the network.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Why does the encoder run fully in parallel?\n",
        "\n",
        "Unlike RNNs:\n",
        "\n",
        "* There is no recurrence\n",
        "* No dependency between time steps inside the encoder\n",
        "\n",
        "This allows:\n",
        "\n",
        "* Massive parallelism on GPUs/TPUs\n",
        "* Faster training\n",
        "* Better scaling to long sequences\n",
        "\n",
        "This was one of the key motivations behind *Attention Is All You Need*.\n",
        "\n",
        "\n",
        "### Why project back to vocabulary with `fc_out`?\n",
        "\n",
        "The model internally operates in a continuous vector space (`d_model`), but the final task is **classification over discrete tokens**.\n",
        "\n",
        "The final linear layer:\n",
        "\n",
        "* Converts hidden states into logits\n",
        "* One logit per vocabulary token\n",
        "* Enables training with cross-entropy loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5jQ4x9i_6pt0"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab,\n",
        "        tgt_vocab,\n",
        "        d_model=512,\n",
        "        n_heads=8,\n",
        "        d_ff=2048,\n",
        "        n_layers=6,\n",
        "        max_len=512\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Source and target token embeddings\n",
        "        #\n",
        "        # These map discrete token IDs to continuous vectors of size d_model.\n",
        "        # Separate embeddings are used because source and target vocabularies\n",
        "        # may differ and play different roles in the model.\n",
        "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        #\n",
        "        # Adds explicit sequence order information to token embeddings.\n",
        "        # This is required because attention itself is permutation-invariant.\n",
        "        self.pos = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Encoder stack\n",
        "        #\n",
        "        # A stack of identical EncoderBlocks.\n",
        "        # Each block refines representations by:\n",
        "        #   1) allowing tokens to attend to each other (self-attention)\n",
        "        #   2) applying non-linear, position-wise transformations (FFN)\n",
        "        self.encoder = nn.ModuleList([\n",
        "            EncoderBlock(d_model, n_heads, d_ff)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder stack\n",
        "        #\n",
        "        # Each DecoderBlock:\n",
        "        #   1) builds an autoregressive representation of the target prefix\n",
        "        #   2) attends to the encoder outputs (cross-attention)\n",
        "        #   3) refines representations with a feedforward network\n",
        "        self.decoder = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Final linear projection\n",
        "        #\n",
        "        # Maps decoder hidden states back to the target vocabulary space,\n",
        "        # producing logits for next-token prediction.\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Source token indices (batch_size, src_len)\n",
        "            tgt: Target token indices (batch_size, tgt_len)\n",
        "            src_mask: Mask for source padding tokens\n",
        "            tgt_mask: Causal mask for target tokens\n",
        "\n",
        "        Returns:\n",
        "            Logits over target vocabulary for each target position\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Embedding + Positional Encoding ---\n",
        "        #\n",
        "        # Token embeddings provide semantic meaning.\n",
        "        # Positional encodings inject order information.\n",
        "        #\n",
        "        # Resulting shape: (batch_size, seq_len, d_model)\n",
        "        src = self.pos(self.src_embed(src))\n",
        "        tgt = self.pos(self.tgt_embed(tgt))\n",
        "\n",
        "        # --- Encoder ---\n",
        "        #\n",
        "        # The encoder processes the entire source sequence in parallel.\n",
        "        # Its output is a contextual representation of the source,\n",
        "        # which will be attended to by the decoder.\n",
        "        for layer in self.encoder:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # --- Decoder ---\n",
        "        #\n",
        "        # The decoder processes the target sequence autoregressively.\n",
        "        # At each layer, it:\n",
        "        #   - attends to past target tokens (masked self-attention)\n",
        "        #   - attends to encoder outputs (cross-attention)\n",
        "        for layer in self.decoder:\n",
        "            tgt = layer(tgt, src, tgt_mask, src_mask)\n",
        "\n",
        "        # --- Output Projection ---\n",
        "        #\n",
        "        # Convert decoder representations into vocabulary logits.\n",
        "        # These logits are typically passed to a softmax during training\n",
        "        # or used for greedy/beam search during inference.\n",
        "        return self.fc_out(tgt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPcd8F8O7iGf"
      },
      "source": [
        "The Transformer first builds a deep, contextual understanding of the input sequence, then generates the output sequence step by step—each time consulting both its past outputs and the encoded input—using only attention and feedforward layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHBv3siy72lL"
      },
      "source": [
        "\n",
        "## Causal Mask\n",
        "\n",
        "### 1. What problem does the causal mask solve?\n",
        "\n",
        "In autoregressive decoding, the model must obey:\n",
        "\n",
        "> **When predicting token *t*, it can only use tokens `0 … t−1`.**\n",
        "\n",
        "Without a causal mask, self-attention would allow every token to see the entire sequence, including **future tokens**, which would:\n",
        "\n",
        "* Break the probabilistic factorization of sequence generation\n",
        "* Cause information leakage during training\n",
        "* Make inference behavior inconsistent with training\n",
        "\n",
        "\n",
        "### 2. Why a *lower-triangular* matrix?\n",
        "\n",
        "Consider a sequence of length 4:\n",
        "\n",
        "```\n",
        "tokens: y0 y1 y2 y3\n",
        "```\n",
        "\n",
        "The causal mask looks like:\n",
        "\n",
        "```\n",
        "1 0 0 0\n",
        "1 1 0 0\n",
        "1 1 1 0\n",
        "1 1 1 1\n",
        "```\n",
        "\n",
        "Row `i` corresponds to **query position `i`**.\n",
        "Column `j` corresponds to **key position `j`**.\n",
        "\n",
        "A `1` means “attention allowed”, a `0` means “blocked”.\n",
        "\n",
        "This structure enforces:\n",
        "\n",
        "* Past → allowed\n",
        "* Present → allowed\n",
        "* Future → blocked\n",
        "\n",
        "\n",
        "\n",
        "### 3. Why not just use zeros and ones directly?\n",
        "\n",
        "The mask itself is **not applied directly** to the output.\n",
        "Instead, it is used inside attention as:\n",
        "\n",
        "```python\n",
        "scores = scores.masked_fill(mask == 0, -∞)\n",
        "```\n",
        "\n",
        "This ensures that:\n",
        "\n",
        "* Softmax assigns **zero probability** to future positions\n",
        "* Gradients do not flow through masked connections\n",
        "\n",
        "\n",
        "\n",
        "### 4. Why add two extra dimensions?\n",
        "\n",
        "Attention scores have shape:\n",
        "\n",
        "```\n",
        "(batch_size, n_heads, query_len, key_len)\n",
        "```\n",
        "\n",
        "By returning a mask of shape:\n",
        "\n",
        "```\n",
        "(1, 1, size, size)\n",
        "```\n",
        "\n",
        "we allow PyTorch to **broadcast** the same causal structure across:\n",
        "\n",
        "* all batches\n",
        "* all attention heads\n",
        "\n",
        "This ensures that causality is enforced **globally and consistently**.\n",
        "\n",
        "\n",
        "### 5. Why is this essential even during training?\n",
        "\n",
        "Even when the full target sequence is known (teacher forcing):\n",
        "\n",
        "* The model must behave **as if the future is unknown**\n",
        "* Otherwise, it learns shortcuts that do not exist at inference time\n",
        "\n",
        "This alignment between training and inference is critical for stable generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UAuo6w1a7W5W"
      },
      "outputs": [],
      "source": [
        "def causal_mask(size):\n",
        "    # Create a square matrix of ones with shape (size, size)\n",
        "    # This represents all possible query–key positions.\n",
        "    mask = torch.ones(size, size)\n",
        "\n",
        "    # Keep only the lower-triangular part of the matrix.\n",
        "    #\n",
        "    # Positions above the diagonal correspond to \"future\" tokens\n",
        "    # (i.e., positions j > i when predicting token i).\n",
        "    #\n",
        "    # torch.tril enforces the causal constraint:\n",
        "    #   token i can attend to tokens {0, ..., i}\n",
        "    #   but NOT to tokens {i+1, ..., size-1}\n",
        "    mask = torch.tril(mask)\n",
        "\n",
        "    # Add two singleton dimensions so the mask can be broadcast\n",
        "    # across:\n",
        "    #   - batch dimension\n",
        "    #   - attention heads\n",
        "    #\n",
        "    # Final shape: (1, 1, size, size)\n",
        "    # This matches attention score tensors of shape:\n",
        "    #   (batch_size, n_heads, seq_len, seq_len)\n",
        "    return mask.unsqueeze(0).unsqueeze(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGwb0gOX8CjD"
      },
      "source": [
        "The causal mask enforces the rule “no peeking into the future” by blocking attention to tokens that haven’t been generated yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "nrr2_zUJ719v",
        "outputId": "ba02466d-13d0-4668-d1c3-74776f065956"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MultiHeadAttention.forward() takes from 2 to 3 positional arguments but 5 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3427891663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, seq_len, vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3742266830.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# which will be attended to by the decoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# --- Decoder ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2555413802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#   Stabilizes activations and training dynamics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         x = self.norm1(\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MultiHeadAttention.forward() takes from 2 to 3 positional arguments but 5 were given"
          ]
        }
      ],
      "source": [
        "model = Transformer(\n",
        "    src_vocab=10000,\n",
        "    tgt_vocab=10000,\n",
        "    d_model=512,\n",
        "    n_heads=8,\n",
        "    n_layers=6\n",
        ")\n",
        "\n",
        "src = torch.randint(0, 10000, (2, 20))\n",
        "tgt = torch.randint(0, 10000, (2, 20))\n",
        "\n",
        "mask = causal_mask(tgt.size(1))\n",
        "out = model(src, tgt, tgt_mask=mask)\n",
        "\n",
        "print(out.shape)  # (batch, seq_len, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLkNbUb8M85"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl4KQr_38dkf"
      },
      "source": [
        "## So... How do we go about generating Text\n",
        "\n",
        "Transformers do not generate text, they generate next-token probability distributions, you decide how to sample from them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgCBfQdZCS5L"
      },
      "source": [
        "# Decoder-Only vs Encoder-Decoder\n",
        "\n",
        "Some LLMs are encoder–decoder, but most modern general-purpose LLMs are decoder-only. The split is very clean once you classify them by what probability they model.\n",
        "\n",
        "You should always start by asking: *what probability distribution am I trying to learn?*\n",
        "\n",
        "For TinyStories, the goal is to model\n",
        "P(x₁, x₂, …, x_T)\n",
        "which factorizes as\n",
        "P(x₁) · P(x₂ | x₁) · P(x₃ | x₁, x₂) · …\n",
        "\n",
        "This is exactly an **autoregressive process**: each token depends only on the tokens before it.\n",
        "\n",
        "A decoder-only transformer is built precisely to model\n",
        "P(x_t | x₁ … x_{t−1})\n",
        "using **masked self-attention**, which prevents the model from seeing future tokens. The architecture directly enforces the correct causal structure.\n",
        "\n",
        "---\n",
        "\n",
        "An encoder–decoder transformer models a different distribution:\n",
        "P(y | x)\n",
        "\n",
        "That only makes sense when there is a separate input sequence x, such as:\n",
        "\n",
        "* translation (French given English)\n",
        "* summarization (summary given document)\n",
        "* QA (answer given question)\n",
        "\n",
        "---\n",
        "\n",
        "TinyStories has no separate input. There is nothing to encode. Adding an encoder would give the model access to the entire story during training, which breaks the causal assumption and creates a mismatch between training and generation.\n",
        "\n",
        "Decoder-only models also guarantee **training–inference alignment**:\n",
        "\n",
        "* during training, the model only sees past tokens\n",
        "* during generation, the model only sees past tokens\n",
        "\n",
        "Encoder–decoder models cannot guarantee this alignment for language modeling without artificial constraints.\n",
        "\n",
        "So the reason is not “because GPT does it”.\n",
        "\n",
        "The real reason is:\n",
        "\n",
        "Architecture follows probability factorization.\n",
        "\n",
        "If you want to model P(x), use a decoder-only transformer.\n",
        "If you want to model P(y|x), use an encoder–decoder transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lnJ8PBTEAak"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RUFzaNcdEUPM",
        "outputId": "62c7c9a2-2566-420d-ae54-7ead1bf0d195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2124M  100 2124M    0     0   128M      0  0:00:16  0:00:16 --:--:-- 66.8M\n"
          ]
        }
      ],
      "source": [
        "! curl \"https://cas-bridge.xethub.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/02e40cc51c59a4bc6c51bd7bc9acda4316e208745be060558eaf500cd14e9f96?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260112%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260112T124634Z&X-Amz-Expires=3600&X-Amz-Signature=a948d01680e37e90762ec67ccadf0d597a9a94e89dd28da762376ed10ed60b41&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=6507eb42423b46492edf979c&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-train.txt%3B+filename%3D%22TinyStoriesV2-GPT4-train.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1768225594&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2ODIyNTU5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDVlOGRhOTYzMjBiMGVmZTQwYWRlN2EvMDJlNDBjYzUxYzU5YTRiYzZjNTFiZDdiYzlhY2RhNDMxNmUyMDg3NDViZTA2MDU1OGVhZjUwMGNkMTRlOWY5NioifV19&Signature=BPYTyJ2CTFy3Jb5MCHF4i%7EEAndHrAJP8bK6GP7tv6REvFarHH3eAur3dyE6w-7eo7PJGKzzeQDodxSYhwHQE95b2RuywZ5DlxTS%7EelkvlI52suIS6vgxa2bkGq5sW7zD1LAzuP3UEoJ1mniA7vq8WbQ2OPWKy%7ET87Zc8ieGiMZ7KoEOy4OpiUhY3SiU3e%7EI43nHwlcEQEGQ4VpRG5OlQNEnOSwSUhm4UlHIvz6gt3smSOCvgCV7l3MTY0CpPU00YwTp7w0NbIaHsTMuuk8N0XBVeAl%7EdE0o1qs3RSZeUeY2grJbVaxlevb657i0R%7E1uHDZ1%7E-ctEGcXRXpMcLqvlkw__&Key-Pair-Id=K2L8F4GPSG1IFC\" -o TinyStoriesV2-GPT4-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d4pSsU6IM3Qj",
        "outputId": "0e9e092a-1c35-4a2c-efa1-15bbbee45c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He sa"
          ]
        }
      ],
      "source": [
        "\n",
        "! head -c 100 TinyStoriesV2-GPT4-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s03VXM5yEBaH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "DATA_PATH = \"TinyStoriesV2-GPT4-train.txt\"  # change if needed\n",
        "MAX_CHARS = 500_000      # limit for quick runs\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BLOCK_SIZE = 512\n",
        "BATCH_SIZE = 128   # or 256 if it fits\n",
        "GRAD_ACCUM = 1\n",
        "D_MODEL    = 256\n",
        "N_HEADS   = 4\n",
        "N_LAYERS  = 4\n",
        "D_FF      = 1024\n",
        "LR        = 3e-4\n",
        "EPOCHS    = 80\n",
        "AMP       = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeUls29fD31o"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YrPko8WmD_c2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tumtynOFmHs"
      },
      "source": [
        "## LanguageModelDataset (TinyStories)\n",
        "\n",
        "### Historical context\n",
        "\n",
        "Early language models (n-gram models, HMMs) modeled text as a probability distribution over sequences using the factorization:\n",
        "\n",
        "> The probability of a sentence is the product of the probability of each word given all previous words.\n",
        "\n",
        "Neural language models (Bengio et al., 2003) adopted the same formulation, and this objective was later carried over to **RNNs, LSTMs**, and eventually **Transformers**.\n",
        "\n",
        "When **Transformer-based language models** (Radford et al., 2018) removed recurrence entirely, they still preserved this exact probabilistic structure. The only requirement was a dataset that could generate many examples of:\n",
        "\n",
        "> “Given a sequence of previous tokens, predict the next token.”\n",
        "\n",
        "This dataset class exists to construct that learning signal from raw tokenized text such as **TinyStories**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why sliding windows?\n",
        "\n",
        "TinyStories is provided as a **long stream of tokens**, not pre-segmented into independent training samples.\n",
        "\n",
        "The design choice here is to use a **sliding window** over the token stream:\n",
        "\n",
        "* Each window provides a fixed-length context (`block_size`)\n",
        "* The target is the same window shifted by one token\n",
        "\n",
        "This idea predates Transformers and was used in:\n",
        "\n",
        "* Feedforward neural language models\n",
        "* RNN-based language models\n",
        "* GPT-style models\n",
        "\n",
        "It ensures:\n",
        "\n",
        "* Efficient use of all tokens\n",
        "* Strong local coherence learning\n",
        "* Compatibility with causal self-attention\n",
        "\n",
        "---\n",
        "\n",
        "### Why predict a sequence instead of a single token?\n",
        "\n",
        "Although the objective is “predict the next token,” the model predicts **the next token at every position simultaneously**.\n",
        "\n",
        "This aligns with:\n",
        "\n",
        "* Parallel training (a core advantage of Transformers)\n",
        "* Causal masking in self-attention\n",
        "* Efficient GPU utilization\n",
        "\n",
        "Each training example teaches the model multiple prediction tasks in one forward pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6RfLKvmUD5IL"
      },
      "outputs": [],
      "source": [
        "class LanguageModelDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        # `data` is a long sequence of token IDs representing TinyStories.\n",
        "        # Example:\n",
        "        #   [12, 45, 891, 34, 78, ...]\n",
        "        #\n",
        "        # `block_size` defines the length of the context window the model\n",
        "        # is allowed to condition on when predicting the next token.\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Each training example requires `block_size + 1` tokens:\n",
        "        #   - `block_size` tokens for the input\n",
        "        #   - 1 additional token for the shifted target\n",
        "        #\n",
        "        # We subtract `block_size` to avoid indexing past the end\n",
        "        # of the token sequence.\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input sequence (context window)\n",
        "        #\n",
        "        # This represents what the model is allowed to see.\n",
        "        # Shape: (block_size,)\n",
        "        x = self.data[idx : idx + self.block_size]\n",
        "\n",
        "        # Target sequence (next-token labels)\n",
        "        #\n",
        "        # This is the same sequence shifted one position to the right.\n",
        "        # For each position t, the model learns to predict y[t]\n",
        "        # given x[0:t].\n",
        "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
        "\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEfwKE24Fo0F"
      },
      "source": [
        "This dataset turns TinyStories into overlapping context–next-token prediction tasks, enforcing the same left-to-right learning objective that underpins all modern autoregressive language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doU-lMRUD2VA"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wljseRwyGGtX"
      },
      "source": [
        "\n",
        "### BPE (Byte Pair Encoding) Tokenization & Data Pipeline (TinyStories)\n",
        "\n",
        "\n",
        "#### Why BPE tokenization?\n",
        "\n",
        "This code uses **Byte Pair Encoding (BPE) tokenization**, a subword tokenization approach that balances vocabulary size and sequence length.\n",
        "\n",
        "Historically:\n",
        "\n",
        "* **BPE was introduced** for text compression (Gage, 1994) and later adapted for NLP (Sennrich et al., 2016)\n",
        "* **Modern language models** (GPT, BERT, etc.) use subword tokenization like BPE to:\n",
        "\n",
        "  * handle rare and unknown words gracefully\n",
        "  * reduce vocabulary size compared to word-level tokenization\n",
        "  * maintain reasonable sequence lengths\n",
        "* BPE became popular in NLP through works like:\n",
        "\n",
        "  * *Sennrich et al., 2016 – \"Neural Machine Translation of Rare Words with Subword Units\"*\n",
        "\n",
        "For **TinyStories**, BPE tokenization is a good choice because:\n",
        "\n",
        "* It handles the full vocabulary efficiently\n",
        "* Reduces sequence length compared to character-level tokenization\n",
        "* Provides a more realistic setup similar to production language models\n",
        "\n",
        "\n",
        "#### Why encode the entire corpus into one long tensor?\n",
        "\n",
        "```python\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "```\n",
        "\n",
        "Instead of treating each story independently, the dataset is represented as a **single continuous stream of tokens**.\n",
        "\n",
        "This aligns with:\n",
        "\n",
        "* Autoregressive language modeling theory\n",
        "* Sliding-window dataset construction\n",
        "* GPT-style training pipelines\n",
        "\n",
        "The model learns:\n",
        "\n",
        "> “Text is a continuous stream where any position can follow any other position.”\n",
        "\n",
        "This assumption simplifies training and works well in practice.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why use `LanguageModelDataset` + DataLoader?\n",
        "\n",
        "This combination:\n",
        "\n",
        "* Converts the token stream into overlapping `(x, y)` training pairs\n",
        "* Enables batching, shuffling, and parallel loading\n",
        "* Decouples data logic from model logic\n",
        "\n",
        "Historically, this design emerged as PyTorch standardized:\n",
        "\n",
        "* `Dataset` for data definition\n",
        "* `DataLoader` for efficient iteration\n",
        "\n",
        "---\n",
        "\n",
        "#### Why shuffle and drop last?\n",
        "\n",
        "```python\n",
        "shuffle=True\n",
        "drop_last=True\n",
        "```\n",
        "\n",
        "* **Shuffling** prevents the model from seeing data in a fixed order, improving generalization\n",
        "* **Dropping the last batch** ensures consistent batch sizes, which simplifies:\n",
        "\n",
        "  * tensor shapes\n",
        "  * attention masking\n",
        "  * GPU efficiency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_lrSvoJw-IeN"
      },
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, text, vocab_size=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Start with character-level vocabulary\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.vocab = chars.copy()\n",
        "\n",
        "        # Create initial merges (character pairs)\n",
        "        self.merges = {}\n",
        "\n",
        "        # Convert text to list of tokens (initially characters)\n",
        "        tokens = list(text)\n",
        "\n",
        "        # Learn BPE merges\n",
        "        for _ in range(vocab_size - len(chars)):\n",
        "            # Count frequency of each adjacent pair\n",
        "            pair_freqs = {}\n",
        "            for i in range(len(tokens) - 1):\n",
        "                pair = (tokens[i], tokens[i + 1])\n",
        "                pair_freqs[pair] = pair_freqs.get(pair, 0) + 1\n",
        "\n",
        "            if not pair_freqs:\n",
        "                break\n",
        "\n",
        "            # Find most frequent pair\n",
        "            best_pair = max(pair_freqs, key=pair_freqs.get)\n",
        "\n",
        "            # Create new token for this pair\n",
        "            new_token = ''.join(best_pair)\n",
        "            self.merges[best_pair] = new_token\n",
        "            self.vocab.append(new_token)\n",
        "\n",
        "            # Merge all occurrences of this pair in the token list\n",
        "            i = 0\n",
        "            while i < len(tokens) - 1:\n",
        "                if (tokens[i], tokens[i + 1]) == best_pair:\n",
        "                    tokens[i] = new_token\n",
        "                    del tokens[i + 1]\n",
        "                else:\n",
        "                    i += 1\n",
        "\n",
        "        # Create mappings\n",
        "        self.stoi = {token: i for i, token in enumerate(self.vocab)}\n",
        "        self.itos = {i: token for token, i in self.stoi.items()}\n",
        "\n",
        "    def encode(self, s):\n",
        "        # Start with character-level encoding\n",
        "        tokens = list(s)\n",
        "\n",
        "        # Apply BPE merges greedily\n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            i = 0\n",
        "            while i < len(tokens) - 1:\n",
        "                pair = (tokens[i], tokens[i + 1])\n",
        "                if pair in self.merges:\n",
        "                    tokens[i] = self.merges[pair]\n",
        "                    del tokens[i + 1]\n",
        "                    changed = True\n",
        "                else:\n",
        "                    i += 1\n",
        "\n",
        "        return [self.stoi[token] for token in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # Convert IDs back to tokens\n",
        "        tokens = [self.itos[i] for i in ids]\n",
        "        return ''.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QNkPqKs7E5Tv",
        "outputId": "bc83343f-586b-414c-ddaa-106017856a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading data...\")\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()[:MAX_CHARS]\n",
        "\n",
        "tokenizer = BPETokenizer(text, vocab_size=1000)\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "\n",
        "# ---------------- SPLIT ----------------\n",
        "split_ratio = 0.9\n",
        "split_idx = int(len(data) * split_ratio)\n",
        "\n",
        "train_data = data[:split_idx]\n",
        "val_data   = data[split_idx:]\n",
        "\n",
        "# ---------------- DATASETS ----------------\n",
        "train_dataset = LanguageModelDataset(train_data, BLOCK_SIZE)\n",
        "val_dataset   = LanguageModelDataset(val_data, BLOCK_SIZE)\n",
        "\n",
        "# ---------------- LOADERS ----------------\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=3,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "     num_workers=2,\n",
        "    shuffle=False,      # IMPORTANT\n",
        "    drop_last=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0caS2FoGF1H"
      },
      "source": [
        "This pipeline converts raw TinyStories text into a stream of BPE subword tokens and then into overlapping context–next-token prediction tasks, exactly matching the autoregressive objective used by GPT-style language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DUyZb7zZJJCG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-cgPW3EgVi"
      },
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDNVFNWWHn7g"
      },
      "source": [
        "### MultiAttentionHead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtIK3RjiIt6a"
      },
      "source": [
        "\n",
        "\n",
        "### Differences vs earlier attention (only real changes)\n",
        "\n",
        "#### 1. **Self-attention only (no Q, K, V inputs)**\n",
        "\n",
        "**Earlier**\n",
        "\n",
        "```python\n",
        "forward(self, Q, K, V, mask=None)\n",
        "```\n",
        "\n",
        "**Now**\n",
        "\n",
        "```python\n",
        "forward(self, x, mask=None)\n",
        "```\n",
        "\n",
        "**What changed**\n",
        "\n",
        "* Q, K, V are all derived from the same tensor `x`\n",
        "* This is **pure self-attention only**\n",
        "\n",
        "**Why**\n",
        "\n",
        "* GPT-style decoder-only models never use cross-attention\n",
        "* Simplifies the API and reduces surface area for bugs\n",
        "* Matches how GPT actually operates\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Attention logic is inlined (no ScaledDotProductAttention module)**\n",
        "\n",
        "**Earlier**\n",
        "\n",
        "```python\n",
        "self.attn = ScaledDotProductAttention(self.d_k)\n",
        "x, _ = self.attn(Q, K, V, mask)\n",
        "```\n",
        "\n",
        "**Now**\n",
        "\n",
        "```python\n",
        "scores = (q @ k.transpose(-2, -1)) / sqrt(d_k)\n",
        "attn = softmax(scores)\n",
        "out = attn @ v\n",
        "```\n",
        "\n",
        "**What changed**\n",
        "\n",
        "* Attention computation is written inline\n",
        "* No separate abstraction\n",
        "\n",
        "**Why**\n",
        "\n",
        "* GPT-style minimalism\n",
        "* Fewer function calls\n",
        "* Easier to fuse / optimize later\n",
        "* Matches most real GPT codebases\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Causal masking assumed externally**\n",
        "\n",
        "**Earlier**\n",
        "\n",
        "* Masking semantics varied (encoder mask, decoder mask, src/tgt)\n",
        "\n",
        "**Now**\n",
        "\n",
        "```python\n",
        "scores = scores.masked_fill(mask == 0, -1e9)\n",
        "```\n",
        "\n",
        "**What changed**\n",
        "\n",
        "* This module assumes the mask is already **causal and broadcastable**\n",
        "* No logic to construct or interpret masks internally\n",
        "\n",
        "**Why**\n",
        "\n",
        "* Cleaner separation of concerns\n",
        "* GPT always uses causal masking\n",
        "* Mask construction belongs to the model, not attention\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **No attention weights returned**\n",
        "\n",
        "**Earlier**\n",
        "\n",
        "```python\n",
        "return output, attn\n",
        "```\n",
        "\n",
        "**Now**\n",
        "\n",
        "```python\n",
        "return self.W_o(out)\n",
        "```\n",
        "\n",
        "**What changed**\n",
        "\n",
        "* Attention weights are discarded\n",
        "\n",
        "**Why**\n",
        "\n",
        "* GPT training never uses attention maps\n",
        "* Saves memory\n",
        "* Reduces bandwidth and overhead\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Single-path projection (GPT style)**\n",
        "\n",
        "This version:\n",
        "\n",
        "* Projects Q/K/V from the **same input**\n",
        "* Concatenates heads\n",
        "* Applies one output projection\n",
        "\n",
        "This matches **GPT-1 → GPT-4** style attention exactly.\n",
        "\n",
        "---\n",
        "\n",
        "## What did *not* change (important)\n",
        "\n",
        "* Scaled dot-product attention\n",
        "* Multi-head splitting\n",
        "* Softmax over keys\n",
        "* Residual compatibility\n",
        "* Parallel attention computation\n",
        "\n",
        "Only the **scope and structure** changed, not the math.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OfsIKQUJHqrS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-style multi-head self-attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # Fused projection for Q, K, V (faster + cleaner)\n",
        "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, T, d_model)\n",
        "            mask: (1 or B, 1 or n_heads, T, T) causal mask (0 = block)\n",
        "\n",
        "        Returns:\n",
        "            (B, T, d_model)\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # Project & split into Q, K, V\n",
        "        # ------------------------------------------------\n",
        "        # (B, T, 3 * C) → (B, T, 3, n_heads, d_k)\n",
        "        qkv = self.qkv_proj(x).view(B, T, 3, self.n_heads, self.d_k)\n",
        "\n",
        "        # (B, n_heads, T, d_k)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # Scaled dot-product attention\n",
        "        # ------------------------------------------------\n",
        "        # (B, n_heads, T, T)\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # (B, n_heads, T, d_k)\n",
        "        out = attn @ v\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # Recombine heads\n",
        "        # ------------------------------------------------\n",
        "        # (B, T, C)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.out_proj(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abqu7ns9IwYv"
      },
      "source": [
        "This version removes generality (cross-attention, modular abstractions) in favor of a minimal, self-attention–only implementation that matches how GPT actually works in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjPHzHLyHFhL"
      },
      "source": [
        "\n",
        "### Earlier Transformer (Vaswani et al., 2017)\n",
        "\n",
        "* Designed for **sequence-to-sequence** tasks (e.g., translation)\n",
        "* Architecture:\n",
        "\n",
        "  * Encoder stack\n",
        "  * Decoder stack\n",
        "  * Cross-attention between them\n",
        "\n",
        "### GPT (Radford et al., 2018 → GPT-4)\n",
        "\n",
        "* Designed for **language modeling and generation**\n",
        "* Architecture:\n",
        "\n",
        "  * **Decoder-only Transformer**\n",
        "  * No encoder\n",
        "  * No cross-attention\n",
        "\n",
        "**Why the change?**\n",
        "\n",
        "Language modeling only requires:\n",
        "\n",
        "> “Predict the next token given all previous tokens.”\n",
        "\n",
        "There is **no source sequence** to condition on, so:\n",
        "\n",
        "* Encoder becomes unnecessary\n",
        "* Cross-attention is removed\n",
        "* Model becomes simpler and more scalable\n",
        "\n",
        "This simplification was one of the key insights behind GPT.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Architectural evolution: Pre-LN instead of Post-LN\n",
        "\n",
        "### Earlier code (Encoder / Decoder blocks)\n",
        "\n",
        "we used **post-norm**:\n",
        "\n",
        "```python\n",
        "x = LayerNorm(x + sublayer(x))\n",
        "```\n",
        "\n",
        "### GPT-style block\n",
        "\n",
        "This implementation uses **pre-norm**:\n",
        "\n",
        "```python\n",
        "x = x + sublayer(LayerNorm(x))\n",
        "```\n",
        "\n",
        "**Why this change?**\n",
        "\n",
        "Research showed that:\n",
        "\n",
        "* Post-LN Transformers become unstable at depth\n",
        "* Gradients struggle to flow through many layers\n",
        "\n",
        "**Pre-LN**:\n",
        "\n",
        "* Improves gradient flow\n",
        "* Allows very deep models (100+ layers)\n",
        "* Is now standard in GPT, LLaMA, PaLM, etc.\n",
        "\n",
        "**Key reference:**\n",
        "Xiong et al., *On Layer Normalization in Transformers*, ICML 2020\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Single attention type: masked self-attention only\n",
        "\n",
        "Earlier:\n",
        "\n",
        "* Encoder self-attention\n",
        "* Decoder masked self-attention\n",
        "* Decoder cross-attention\n",
        "\n",
        "GPT:\n",
        "\n",
        "* **Only masked self-attention**\n",
        "\n",
        "```python\n",
        "self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "```\n",
        "\n",
        "with:\n",
        "\n",
        "```python\n",
        "mask = causal_mask(T)\n",
        "```\n",
        "\n",
        "**Why?**\n",
        "\n",
        "* GPT models a single sequence\n",
        "* Causality enforces correct generation\n",
        "* Cross-attention is unnecessary\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Weight tying (embedding ↔ output projection)\n",
        "\n",
        "```python\n",
        "self.head.weight = self.embed.weight\n",
        "```\n",
        "\n",
        "This was **not in the original Transformer**, but introduced and popularized in:\n",
        "\n",
        "* Press & Wolf (2017)\n",
        "* GPT-1 onward\n",
        "\n",
        "**Why weight tying?**\n",
        "\n",
        "* Reduces parameter count\n",
        "* Improves generalization\n",
        "* Enforces symmetry between:\n",
        "\n",
        "  * “reading” a token (embedding)\n",
        "  * “writing” a token (logits)\n",
        "\n",
        "This is now standard practice.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Final LayerNorm before output\n",
        "\n",
        "```python\n",
        "self.ln_f = nn.LayerNorm(D_MODEL)\n",
        "```\n",
        "\n",
        "This **final normalization layer**:\n",
        "\n",
        "* Stabilizes logits\n",
        "* Improves training dynamics\n",
        "* Became standard in GPT-style models\n",
        "\n",
        "Earlier Transformers normalized **inside** blocks only.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Positional encoding remains (but later evolved)\n",
        "\n",
        "This code still uses **sinusoidal positional encoding**, but historically:\n",
        "\n",
        "* GPT-1 / GPT-2 → learned positional embeddings\n",
        "* GPT-3+ → RoPE / variants\n",
        "* LLaMA → RoPE\n",
        "* ALiBi → attention bias instead of embeddings\n",
        "\n",
        "But the conceptual role remains unchanged:\n",
        "\n",
        "> Inject order into an otherwise permutation-invariant attention mechanism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "45NcGdveEg9R"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        # Masked multi-head self-attention\n",
        "        # Uses causal masking to prevent access to future tokens\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Position-wise feedforward network\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "\n",
        "        # Pre-layer normalization\n",
        "        # Improves gradient flow in deep networks\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Pre-LN + residual connection for attention\n",
        "        # x ← x + Attention(LayerNorm(x))\n",
        "        x = x + self.attn(self.ln1(x), mask)\n",
        "\n",
        "        # Pre-LN + residual connection for feedforward\n",
        "        # x ← x + FFN(LayerNorm(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r6RIrgvlGgT4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Token embedding layer\n",
        "        # Maps token IDs to continuous vectors\n",
        "        self.embed = nn.Embedding(vocab_size, D_MODEL)\n",
        "\n",
        "        # Positional encoding to inject order information\n",
        "        self.pos = PositionalEncoding(D_MODEL, BLOCK_SIZE)\n",
        "\n",
        "        # Stack of Transformer blocks\n",
        "        # Each block refines representations autoregressively\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(D_MODEL, N_HEADS, D_FF)\n",
        "            for _ in range(N_LAYERS)\n",
        "        ])\n",
        "\n",
        "        # Final LayerNorm before output projection\n",
        "        self.ln_f = nn.LayerNorm(D_MODEL)\n",
        "\n",
        "        # Output projection to vocabulary space\n",
        "        self.head = nn.Linear(D_MODEL, vocab_size)\n",
        "\n",
        "        # Weight tying between input embeddings and output logits\n",
        "        # Reduces parameters and improves generalization\n",
        "        self.head.weight = self.embed.weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input token IDs (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Logits over vocabulary for each position\n",
        "        \"\"\"\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        # Create causal mask to enforce autoregressive constraint\n",
        "        mask = causal_mask(T).to(x.device)\n",
        "\n",
        "        # Token embeddings + positional encodings\n",
        "        x = self.embed(x)\n",
        "        x = self.pos(x)\n",
        "\n",
        "        # Apply stacked Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Final normalization and projection to logits\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvli5PvzHDjG"
      },
      "source": [
        "GPT removes the encoder, enforces causality everywhere, stabilizes training with pre-norm, and scales depth—turning the Transformer into a pure next-token prediction machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3pX7inOE-Lt"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Y36DT6EZEhXF"
      },
      "outputs": [],
      "source": [
        "model = GPT(tokenizer.vocab_size).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VO17K0fIZ49w",
        "outputId": "d424c0b4-020a-4272-d249-c220b3e63501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13858 KiB |  13858 KiB |  13858 KiB |      0 B   |\n",
            "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 B   |\n",
            "|       from small pool |  13858 KiB |  13858 KiB |  13858 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13858 KiB |  13858 KiB |  13858 KiB |      0 B   |\n",
            "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 B   |\n",
            "|       from small pool |  13858 KiB |  13858 KiB |  13858 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13857 KiB |  13857 KiB |  13857 KiB |      0 B   |\n",
            "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 B   |\n",
            "|       from small pool |  13857 KiB |  13857 KiB |  13857 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14336 KiB |  14336 KiB |  14336 KiB |      0 B   |\n",
            "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 B   |\n",
            "|       from small pool |  14336 KiB |  14336 KiB |  14336 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 489472 B   |   1816 KiB |   7704 KiB |   7226 KiB |\n",
            "|       from large pool |      0 B   |      0 KiB |      0 KiB |      0 KiB |\n",
            "|       from small pool | 489472 B   |   1816 KiB |   7704 KiB |   7226 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |      53    |      53    |      53    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |      53    |      53    |      53    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |      53    |      53    |      53    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |      53    |      53    |      53    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       7    |       7    |       7    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       2    |       4    |       7    |       5    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       2    |       4    |       7    |       5    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "print(torch.cuda.memory_summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Compile model (PyTorch 2.x)\n",
        "model = torch.compile(model)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "ACC_STEPS = 4          # 🔧 tune this\n",
        "CLIP_NORM = 1.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vbsnDerNSMX7",
        "outputId": "1a2a340e-b6dd-41c1-eb26-73afc701668b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1351142244.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lzLzYgS3FDzN"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(prompt, max_new_tokens=300, temperature=0.8):\n",
        "    model.eval()\n",
        "    ids = torch.tensor([tokenizer.encode(prompt)], device=DEVICE)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Truncate input to BLOCK_SIZE if it exceeds it.\n",
        "        # The model was trained with BLOCK_SIZE context.\n",
        "        input_ids = ids[:, -BLOCK_SIZE:]\n",
        "        logits = model(input_ids)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, 1)\n",
        "        ids = torch.cat([ids, next_id], dim=1)\n",
        "\n",
        "    return tokenizer.decode(ids[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "o38PTqXXE_RY",
        "outputId": "62364cd9-183c-4acf-b525-2c93c6df36e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-960699167.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from best_model.pt\n",
            "Loaded weights-only checkpoint (optimizer/scaler reset)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 1:   0%|          | 0/1539 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-960699167.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 1: 100%|██████████| 1539/1539 [04:39<00:00,  5.51it/s]\n",
            "/tmp/ipython-input-960699167.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Train 2.9840 | Val 2.8299  ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 2: 100%|██████████| 1539/1539 [04:47<00:00,  5.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2 | Train 2.0371 | Val 2.9852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 3: 100%|██████████| 1539/1539 [04:46<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3 | Train 1.2778 | Val 4.0615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 4: 100%|██████████| 1539/1539 [04:46<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4 | Train 0.7066 | Val 5.6822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 5: 100%|██████████| 1539/1539 [04:45<00:00,  5.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Train 0.3048 | Val 7.1991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 6:   5%|▍         | 74/1539 [00:13<04:36,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-960699167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             loss = criterion(\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     def __reduce__(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcompile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3997439918.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \"\"\"\n\u001b[1;32m     31\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_buffers_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;31m# Just for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mruntime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    337\u001b[0m             ):\n\u001b[1;32m    338\u001b[0m                 \u001b[0mrecord_runtime_wrapper_prologue_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 all_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteal_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   2116\u001b[0m                 \u001b[0;31m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m                 \u001b[0;31m#   in the fw output order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m                 fw_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m   2119\u001b[0m                     \u001b[0mCompiledFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_fw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36minner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mold_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;31m# Inductor cache DummyModule can return None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 \u001b[0;34mf\"## Call CompiledFxGraph {self._fx_graph_cache_key} ##\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             ):\n\u001b[0;32m--> 613\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mget_runtime_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2960\u001b[0m             \u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated_input_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m         )\n\u001b[0;32m-> 2962\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[0;31m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/sv/csv6gsksefxxg6scu3da4kck365wrohwwptodqbb3nc7lfydr7tn.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mbuf97\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_strided_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0;31m# Topologically Sorted Source Nodes: [layer_norm_6, linear_12], Original ATen: [aten.native_layer_norm, aten._to_copy, aten.view, aten.addmm]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0mextern_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf97\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m             \u001b[0mbuf98\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf89\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m131072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mbuf89\u001b[0m  \u001b[0;31m# reuse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0;31m# Topologically Sorted Source Nodes: [linear_12, qkv_3, unbind_3, q_7, matmul_6], Original ATen: [aten._to_copy, aten.addmm, aten.view, aten.unbind, aten.transpose, aten.clone]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ============================================================\n",
        "# RESUME FROM CHECKPOINT (FULL STATE)\n",
        "# ============================================================\n",
        "\n",
        "best_val_loss = math.inf\n",
        "start_epoch = 0\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(\"best_model.pt\"):\n",
        "    print(\"Resuming from best_model.pt\")\n",
        "    ckpt = torch.load(\"best_model.pt\", map_location=DEVICE)\n",
        "\n",
        "    if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
        "        # ✅ NEW-style checkpoint\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "        best_val_loss = ckpt[\"best_val_loss\"]\n",
        "        start_epoch = ckpt[\"epoch\"] + 1\n",
        "    else:\n",
        "        # ✅ OLD-style checkpoint (weights only)\n",
        "        model.load_state_dict(ckpt)\n",
        "        print(\"Loaded weights-only checkpoint (optimizer/scaler reset)\")\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "\n",
        "    # ===================== TRAIN =====================\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for step, (x, y) in enumerate(tqdm(train_loader, desc=f\"Train {epoch+1}\")):\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with autocast():\n",
        "            logits = model(x)\n",
        "            loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            loss = loss / ACC_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        train_loss += loss.item() * ACC_STEPS\n",
        "\n",
        "        should_step = (\n",
        "            (step + 1) % ACC_STEPS == 0\n",
        "            or (step + 1) == len(train_loader)  # flush last batch\n",
        "        )\n",
        "\n",
        "        if should_step:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # ===================== VALIDATE =====================\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    MAX_VAL_BATCHES = 200\n",
        "\n",
        "    with torch.no_grad(), autocast():\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            if i >= MAX_VAL_BATCHES:\n",
        "                break\n",
        "\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                y.view(-1)\n",
        "            )\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= min(len(val_loader), MAX_VAL_BATCHES)\n",
        "\n",
        "    # ===================== LOGGING + CHECKPOINT =====================\n",
        "    improved = val_loss < best_val_loss\n",
        "    if improved:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scaler\": scaler.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"best_val_loss\": best_val_loss,\n",
        "            },\n",
        "            \"best_model.pt\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:3d} | \"\n",
        "        f\"Train {train_loss:.4f} | \"\n",
        "        f\"Val {val_loss:.4f}\"\n",
        "        + (\"  ✓\" if improved else \"\")\n",
        "    )\n",
        "\n",
        "    # ===================== SAMPLE =====================\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        prompt = torch.tensor([[tokenizer.stoi.get(\"O\", 0)]], device=DEVICE)\n",
        "        gen = model.generate(prompt, max_tokens=100)\n",
        "        print(f\"\\nSample:\\n{tokenizer.decode(gen[0].tolist())}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# DONE\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(f\"Training complete! Best val loss: {best_val_loss:.4f}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTSoehxrFDOn"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vehZdvH1FGDG"
      },
      "source": [
        "### Drumroll ....\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sHwNJPjxFHgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "010b1d88-32d2-426c-e620-9d31aa6686b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- GENERATED STORY ---\n",
            "\n",
            "Once upon a time  was tirered. It made go to the park, but he also lion named sed sed with a new friend, a little girl named Lily him. She saw her frog and opped her head. Lily was very munice. Then, She camemmembered Mom comineace braceful was not too too happy ve fun.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little girl named Mia. Mia had a big, red in a little house with her  always smiled. Mia truckly little cat the ad. Lily lived in a small house with her momy. Mia loved to stay very happy.\n",
            "Mia went outside to play in the stawn and saw a lotched the slide. She was very happy and s. They cyedguit. The slizzlep. Mia could not find it was very happy. The kids girl \n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- GENERATED STORY ---\\n\")\n",
        "print(generate(\"Once upon a time  \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 3.1M-parameter model is very small by modern language-model standards. For context, classic “toy” GPT implementations used for learning typically range from **1–10M parameters**, which is enough to capture basic token statistics and short-range patterns but not sustained coherence. GPT-2 Small already has **117M parameters** (≈40× larger), GPT-2 Medium **345M**, GPT-2 Large **774M**, and GPT-2 XL **1.5B**. Modern production LLMs operate in the **tens to hundreds of billions** of parameters. Practically, this means a 3.1M model is expected to produce locally sensible text but struggle with long-term consistency, reasoning, and abstraction\n",
        "\n",
        "This is  ideal for understanding *how* transformers work, not for demonstrating strong language capability.\n"
      ],
      "metadata": {
        "id": "bnL6gqh4oiHq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5h06Y80of5YA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}